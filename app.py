# app.py (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå: ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î Google Drive + ‡∏õ‡∏∏‡πà‡∏°‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å)

import streamlit as st
import pandas as pd
from datetime import datetime
import gspread 
import io 
# --- Imports ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Google Drive API ---
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseUpload 
# ----------------------------------------

# --- 1. Global Configuration ---
# Google Sheet ID ‡πÅ‡∏•‡∏∞ Worksheet Name
GOOGLE_SHEET_ID = "1E6WpIgmUBZ2bPpBxSW08ktKUKJGahmzqjVcMDfsqMec"
WORKSHEET_NAME = "FactoryAudit"

# ‚úÖ ‡∏£‡∏´‡∏±‡∏™ Folder ID ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå (‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏´‡πâ‡∏°‡∏≤)
RecievedFromFactory = r"C:\Users\onuma.l\OneDrive - Hi-Tech Apparel Co.,Ltd\PDU - Product Development Unit\PDU Information\10-Heat Transfer Project\07-Heat Transfer Audit"

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
SCORE_MAPPING = {
    'OK': 3, 'PRN': 2, 'NRIC': 1, 'Blank': 0 
}

MAIN_CATEGORIES = [
    "1. People (‡∏ö‡∏∏‡∏Ñ‡∏•‡∏≤‡∏Å‡∏£)", "2. Machine (‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏±‡∏Å‡∏£)", "3. Materials (‡∏ß‡∏±‡∏™‡∏î‡∏∏)", "4. Method (‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£)", 
    "5. Measurement (‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î)", "6. Environment (‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°)", "7. Documentation & Control (‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°)"
]

CATEGORY_ID_MAP = {
    '1': "1. People (‡∏ö‡∏∏‡∏Ñ‡∏•‡∏≤‡∏Å‡∏£)", '2': "2. Machine (‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏±‡∏Å‡∏£)", '3': "3. Materials (‡∏ß‡∏±‡∏™‡∏î‡∏∏)", 
    '4': "4. Method (‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£)", '5': "5. Measurement (‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î)", '6': "6. Environment (‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°)", 
    '7': "7. Documentation & Control (‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°)"
}

def get_grade_and_description(percentage):
    if percentage >= 90:
        return 'A', 'Excellent (‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°)', '‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠'
    elif percentage >= 75:
        return 'B', 'Good (‡∏î‡∏µ)', '‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÑ‡∏î‡πâ‡∏î‡∏µ ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û'
    elif percentage >= 60:
        return 'C', 'Fair (‡∏û‡∏≠‡πÉ‡∏ä‡πâ)', '‡∏°‡∏µ‡∏ö‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•'
    else:
        return 'D', 'Poor (‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô)', '‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏´‡∏•‡∏±‡∏Å ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏ã‡πâ‡∏≥'

def process_checklist_data(uploaded_file):
    try:
        uploaded_file.seek(0)
        if uploaded_file.name.endswith('.xlsx'):
            df_metadata = pd.read_excel(uploaded_file, nrows=15, header=None)
        else:
            df_metadata = pd.read_csv(uploaded_file, nrows=15, header=None)
        
        metadata_raw = {
            'Date_of_Audit': df_metadata.iloc[3, 2],
            'Time_Shift': df_metadata.iloc[3, 5],
            'Factory': df_metadata.iloc[4, 2],
            'Work_Area': df_metadata.iloc[4, 5],
            'Observed_Personnel': df_metadata.iloc[5, 2],
            'Supervisor': df_metadata.iloc[5, 5],
            'Machine_ID': df_metadata.iloc[6, 2],
            'Auditor': df_metadata.iloc[6, 5],
            'File_Name': uploaded_file.name
        }
    except Exception as e:
        metadata_raw = {k: 'N/A' for k in ['Date_of_Audit', 'Time_Shift', 'Factory', 'Work_Area', 'Observed_Personnel', 'Supervisor', 'Machine_ID', 'Auditor']}
        metadata_raw['File_Name'] = uploaded_file.name

    try:
        uploaded_file.seek(0) 
        col_indices = [1, 2, 3, 5, 6, 7, 8] 
        if uploaded_file.name.endswith('.xlsx'):
            df_audit = pd.read_excel(uploaded_file, header=13, usecols=col_indices)
        else:
            df_audit = pd.read_csv(uploaded_file, header=13, usecols=col_indices)
        
        df_audit.columns = ['‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠', '‡πÄ‡∏•‡∏Ç‡∏Ç‡πâ‡∏≠', '‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°', 'OK', 'PRN', 'NRIC', '‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏']
        df_audit = df_audit.dropna(subset=['‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°']).copy() 
        df_audit['Category_ID'] = df_audit['‡πÄ‡∏•‡∏Ç‡∏Ç‡πâ‡∏≠'].astype(str).str.split('.', expand=True)[0]
        df_audit = df_audit[df_audit['Category_ID'].isin(CATEGORY_ID_MAP.keys())].reset_index(drop=True)
        df_audit['‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠'] = df_audit['‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠'].ffill() 
        
    except Exception as e:
        st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {e}")
        return None, None, None

    df_audit['Score'] = 0
    df_audit['Scoring Category'] = 'Blank'

    for index, row in df_audit.iterrows():
        if pd.notna(row['OK']) and row['OK'] != "":
            df_audit.loc[index, 'Score'] = SCORE_MAPPING['OK']
            df_audit.loc[index, 'Scoring Category'] = 'OK'
        elif pd.notna(row['PRN']) and row['PRN'] != "":
            df_audit.loc[index, 'Score'] = SCORE_MAPPING['PRN']
            df_audit.loc[index, 'Scoring Category'] = 'PRN'
        elif pd.notna(row['NRIC']) and row['NRIC'] != "":
            df_audit.loc[index, 'Score'] = SCORE_MAPPING['NRIC']
            df_audit.loc[index, 'Scoring Category'] = 'NRIC'

    df_audited_q = df_audit[df_audit['Score'] > 0]
    total_possible_questions = len(df_audited_q) 
    actual_score = df_audited_q['Score'].sum()
    total_possible_score = total_possible_questions * SCORE_MAPPING['OK'] 
    percentage = (actual_score / total_possible_score) * 100 if total_possible_score > 0 else 0
    grade, grade_level, description = get_grade_and_description(percentage)

    group_scores_detailed = {}
    if 'Category_ID' in df_audited_q.columns:
        for category_id, group_df in df_audited_q.groupby('Category_ID'):
            group_full_name = CATEGORY_ID_MAP.get(category_id, 'Unknown')
            group_name = group_full_name.split('.', 1)[-1].strip().replace(' ', '_').replace('/', '_').replace('&', '').strip()
            group_score = group_df['Score'].sum()
            max_group_score = len(group_df) * SCORE_MAPPING['OK']
            group_remarks_list = group_df['‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏'].dropna().tolist()
            group_remarks_text = " / ".join(group_remarks_list)
            
            group_scores_detailed[f'Score_{group_name}'] = f"{group_score}/{max_group_score}"
            group_scores_detailed[f'Score_{group_name}_Actual'] = group_score
            group_scores_detailed[f'Score_{group_name}_Max'] = max_group_score
            group_scores_detailed[f'Remarks_{group_name}'] = group_remarks_text
            
    final_summary = {
        'Timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'Date_of_Audit': metadata_raw['Date_of_Audit'],
        'Time_Shift': metadata_raw['Time_Shift'],
        'Factory': metadata_raw['Factory'],
        'Work_Area': metadata_raw['Work_Area'],
        'Observed_Personnel': metadata_raw['Observed_Personnel'],
        'Supervisor': metadata_raw['Supervisor'],
        'Machine_ID': metadata_raw['Machine_ID'],
        'Auditor': metadata_raw['Auditor'],
        'File_Name': metadata_raw['File_Name'],
        'Actual_Score': actual_score,
        'Score_Percentage_pct': round(percentage, 2),
        'Grade': grade,
        'Grade_Level': grade_level,
        'Description': description,
        'Total_Questions_Audited': total_possible_questions,
        'Max_Possible_Score': total_possible_score,
    }
    final_summary.update(group_scores_detailed)
    return df_audit, final_summary, df_audited_q

# --- 3. GOOGLE SHEETS & DRIVE INTEGRATION ---

def upload_file_to_drive(uploaded_file, folder_id):
    """‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏õ‡∏¢‡∏±‡∏á Google Drive"""
    try:
        credentials_dict = st.secrets["gcp_service_account"]
        SCOPES = ['https://www.googleapis.com/auth/drive.file']
        credentials = service_account.Credentials.from_service_account_info(credentials_dict, scopes=SCOPES)
        
        drive_service = build('drive', 'v3', credentials=credentials)
        
        file_metadata = {
            'name': uploaded_file.name,
            'parents': [folder_id]
        }

        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Upload
        fh = io.BytesIO(uploaded_file.getvalue())
        mimetype = uploaded_file.type if uploaded_file.type else 'application/octet-stream'
        media = MediaIoBaseUpload(fh, mimetype=mimetype, resumable=True)

        # ‡∏™‡∏±‡πà‡∏á Upload (supportsAllDrives=True ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Shared Drive)
        file = drive_service.files().create(
            body=file_metadata,
            media_body=media,
            fields='id',
            supportsAllDrives=True 
        ).execute()
        
        return True, f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏•‡∏á Drive ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (File ID: {file.get('id')})"
    except Exception as e:
        return False, f"‚ùå Error GDrive Upload: {e}"

def automate_storage_and_save(summary_data, uploaded_file):
    
    # 1. ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏õ‡∏¢‡∏±‡∏á Google Drive
    drive_success, drive_message = upload_file_to_drive(uploaded_file, GDRIVE_FOLDER_ID)
    
    if not drive_success:
        return False, drive_message 

    # 2. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Google Sheets
    try:
        gc = gspread.service_account_from_dict(st.secrets["gcp_service_account"])
        spreadsheet = gc.open_by_key(GOOGLE_SHEET_ID)
        worksheet = spreadsheet.worksheet(WORKSHEET_NAME) 

        headers = list(summary_data.keys())
        values = []
        for v in summary_data.values():
            if isinstance(v, (pd.Timestamp, datetime)):
                values.append(str(v))
            elif hasattr(v, 'item'):
                values.append(v.item())
            else:
                values.append(v)

        if worksheet.row_values(1) != headers:
            worksheet.append_row(headers)

        worksheet.append_row(values)
        
        sheet_message = f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÉ‡∏ô Sheet: **{WORKSHEET_NAME}**"
        final_message = f"‚úÖ **‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå:** {drive_message} <br> {sheet_message}"
        return True, final_message

    except KeyError:
        return False, "‚ùå **Error:** ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ `secrets.toml` ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á!"
    except Exception as e:
        return False, f"‚ùå Error GSheets Save: {e}"

# --- 4. Streamlit UI ---

st.set_page_config(layout="wide", page_title="Heat Transfer Audit App")
st.title("üî• ‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô Heat Transfer Process Audit")
st.markdown("---")

# 1. Upload
st.header("1. Upload Heat Transfer Checklist File")
uploaded_file = st.file_uploader(
    "‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡πâ‡∏ß (.xlsx ‡∏´‡∏£‡∏∑‡∏≠ .csv)",
    type=["xlsx", "csv"]
)

if uploaded_file is not None:
    st.success(f"Upload successful: **{uploaded_file.name}**")
    
    # 2. Processing
    df_audit_result, summary, df_audited_q = process_checklist_data(uploaded_file)

    if df_audit_result is not None:
        st.markdown("---")
        # 2. Overall Score
        st.header("2. Overall Score Evaluation (‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°)")
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Actual Score", f"{summary['Actual_Score']}", f"‡∏à‡∏≤‡∏Å {summary['Max_Possible_Score']}")
        col2.metric("Total Score", f"{summary['Max_Possible_Score']}")
        col3.metric("Percentage", f"{summary['Score_Percentage_pct']}%")
        col4.metric("Grade", f"{summary['Grade']} ({summary['Grade_Level']})")
        st.info(f"**Description:** {summary['Description']}")
        
        st.markdown("---")
        
        # 3. Metadata Display (Table)
        st.header("3. Information (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ)")
        METADATA_HEADERS_MAP = {
            'Date of Audit (‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö)': 'Date of Audit\n(‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö)',
            'Time of Audit (‡πÄ‡∏ß‡∏•‡∏≤/‡∏£‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô)': 'Time of Audit\n(‡πÄ‡∏ß‡∏•‡∏≤/‡∏£‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô)',
            'Factory (‡πÇ‡∏£‡∏á‡∏á‡∏≤‡∏ô)': 'Factory\n(‡πÇ‡∏£‡∏á‡∏á‡∏≤‡∏ô)',
            'Work Area (‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö)': 'Work Area\n(‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö)',
            'Machine ID (‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏±‡∏Å‡∏£)': 'Machine ID\n(‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏±‡∏Å‡∏£)',
            'Auditor (‡∏ú‡∏π‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö)': 'Auditor\n(‡∏ú‡∏π‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö)',
            'Observed Personnel (‡∏ú‡∏π‡πâ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏á‡∏≤‡∏ô)': 'Observed Personnel\n(‡∏ú‡∏π‡πâ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏á‡∏≤‡∏ô)',
            'Supervisor (‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏á‡∏≤‡∏ô)': 'Supervisor\n(‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏á‡∏≤‡∏ô)',
            'File Name (‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î)': 'File Name\n(‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î)',
        }
        metadata_map = {
            'Date of Audit (‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö)': summary.get('Date_of_Audit'),
            'Time of Audit (‡πÄ‡∏ß‡∏•‡∏≤/‡∏£‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô)': summary.get('Time_Shift'),
            'Factory (‡πÇ‡∏£‡∏á‡∏á‡∏≤‡∏ô)': summary.get('Factory'),
            'Work Area (‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö)': summary.get('Work_Area'),
            'Machine ID (‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏±‡∏Å‡∏£)': summary.get('Machine_ID'),
            'Auditor (‡∏ú‡∏π‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö)': summary.get('Auditor'),
            'Observed Personnel (‡∏ú‡∏π‡πâ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏á‡∏≤‡∏ô)': summary.get('Observed_Personnel'),
            'Supervisor (‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏á‡∏≤‡∏ô)': summary.get('Supervisor'),
            'File Name (‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î)': summary.get('File_Name'),
        }
        df_metadata_table = pd.DataFrame(metadata_map.items(), columns=['Internal Header', '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'])
        df_metadata_table['Header (‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠)'] = df_metadata_table['Internal Header'].apply(lambda x: METADATA_HEADERS_MAP.get(x, x))
        st.dataframe(df_metadata_table[['Header (‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠)', '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•']], hide_index=True, use_container_width=True)

        st.markdown("---")
        
        # 4. Summary & Details
        st.header("4. Summary & Details (‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î)")
        with st.expander("‡∏î‡∏π‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 7 ‡∏î‡πâ‡∏≤‡∏ô", expanded=True):
            group_summary_data = []
            for category_th in MAIN_CATEGORIES:
                key_name = category_th.split('.', 1)[-1].strip().replace(' ', '_').replace('&', '').strip()
                actual = summary.get(f'Score_{key_name}_Actual', 0)
                max_score = summary.get(f'Score_{key_name}_Max', 0)
                remarks_text = summary.get(f'Remarks_{key_name}', '')
                percentage = (actual / max_score) * 100 if max_score > 0 else 0
                group_summary_data.append({
                    'Main Category': category_th.replace(' (', '\n('), 
                    'Score': f"{actual} / {max_score}", 
                    'Percentage (%)': f"{percentage:.2f}%", 
                    'Remark': remarks_text
                })
            st.dataframe(pd.DataFrame(group_summary_data), hide_index=True, use_container_width=True)

        with st.expander("‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠"):
            DISPLAY_COLUMNS_MAP = {
                '‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠': 'Category',
                '‡πÄ‡∏•‡∏Ç‡∏Ç‡πâ‡∏≠': 'No.',
                '‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°': 'Question',
                'OK': 'OK',
                'PRN': 'PRN',
                'NRIC': 'NRIC',
                '‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏': 'Remark'
            }
            df_display = df_audit_result[['‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠', '‡πÄ‡∏•‡∏Ç‡∏Ç‡πâ‡∏≠', '‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°', 'OK', 'PRN', 'NRIC', '‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏']].copy()
            cols_to_clean = ['OK', 'PRN', 'NRIC', '‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏']
            df_display[cols_to_clean] = df_display[cols_to_clean].fillna('')
            df_display['‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠'] = df_display['‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠'].mask(df_display['‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠'].duplicated(), '')
            df_display = df_display.rename(columns=DISPLAY_COLUMNS_MAP)
            st.dataframe(df_display, column_order=list(DISPLAY_COLUMNS_MAP.values()), hide_index=True, use_container_width=True)

        st.markdown("---")
        
        # ----------------------------------------------------
        # 6. ACTION BUTTONS (‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏∏‡πà‡∏°‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡πâ)
        # ----------------------------------------------------
        st.header("6. Actions (‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•)")
        
        col_btn1, col_btn2 = st.columns([1, 1])

        with col_btn1:
            # ‡∏õ‡∏∏‡πà‡∏°‡∏´‡∏•‡∏±‡∏Å: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            if st.button("üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á Google Sheet & Drive", type="primary", use_container_width=True):
                with st.spinner('‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Google Drive ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...'):
                    success, message = automate_storage_and_save(summary, uploaded_file)
                if success:
                    st.success(message)
                    st.balloons()
                else:
                    st.error(message)

        with col_btn2:
            # ‡∏õ‡∏∏‡πà‡∏°‡πÄ‡∏™‡∏£‡∏¥‡∏°: ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà
            if st.button("üîÑ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡∏°‡πà (Reset)", use_container_width=True):
                st.rerun()

        # ‡∏™‡πà‡∏ß‡∏ô‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î (Backup)
        st.download_button(
            label="‚¨áÔ∏è Download Result CSV (‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)",
            data=df_audit_result.to_csv(index=False).encode('utf-8'),
            file_name=f"audit_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )
else:
    st.info("Please upload the filled-out Excel/CSV file to begin evaluation.")
